{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main analysis\n",
    "This notebooks carries out the main analysis of excess mortality\n",
    "\n",
    "The notebooks goes through each county-file (for each possible period), and determines a mortality baseline using the functions in the ExcessMortalityFunctions repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saveFigures is set to: True\n",
      "Done loading packages\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "# Load style\n",
    "plt.style.use('PlotStyle.mplstyle')\n",
    "import matplotlib.colors as colors\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Dark2.colors)\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "# Load functions\n",
    "import sys\n",
    "sys.path.append(\"../../ExcessMortality\")\n",
    "import ExcessMortalityFunctions as emf\n",
    "# import AdditionalFunctions as ps\n",
    "\n",
    "\n",
    "saveFigures = True\n",
    "# saveFigures = False\n",
    "print('saveFigures is set to: '+str(saveFigures))\n",
    "print('Done loading packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "pathData = '../Data/MortalityCollections/'\n",
    "# pathToSaveResultsIn = '../../AnalysisResults' # NOTE: Outside repo! This is done to save space within repo. In the final part of the analysis, everything is put together in a single file\n",
    "pathToSaveResultsIn = '../Data/AnalysisResults' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRel = pd.read_csv('../SupplementaryTable_RelationalTable_ParishCounty.csv')\n",
    "dfRel['StartDate'] = pd.to_datetime(dfRel.StartDate)\n",
    "dfRel['EndDate'] = pd.to_datetime(dfRel.EndDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRel = dfRel.rename(columns={\n",
    "#     'fra':'StartDate',\n",
    "#     'til':'EndDate',\n",
    "# })\n",
    "\n",
    "# dfRel.to_csv('../SupplementaryTable_RelationalTable_ParishCounty.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "27\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Only consider counties that exists after 1810 \n",
    "# (A large restructuring was done between 1800 and 1810)\n",
    "# print(dfRel.AmtID.unique())\n",
    "print(len(dfRel.AmtID.unique()))\n",
    "allAmtIDs = dfRel[dfRel.EndDate > np.datetime64('1810-01-01')].AmtID.unique()\n",
    "# print(allAmtIDs)\n",
    "dfRel = dfRel[dfRel.AmtID.isin(allAmtIDs)]\n",
    "print(len(allAmtIDs))\n",
    "\n",
    "# Drop Flensborg and Løgumkloster counties (since they only contains two parishes)\n",
    "dfRel = dfRel.drop(dfRel[dfRel.AmtName.str.contains('Løgum')].index) \n",
    "dfRel = dfRel.drop(dfRel[dfRel.AmtName.str.contains('Flensbo')].index) \n",
    "\n",
    "allAmtIDs = dfRel.AmtID.unique()\n",
    "print(len(allAmtIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAmtName(amtID):\n",
    "    # Gets the name of a given county from the ID\n",
    "    return dfRel[dfRel.AmtID == amtID].AmtName.values[0]\n",
    "\n",
    "def getAmtCollections(amtID):\n",
    "    # Function for getting all data-collections for a specific county, as well as the periods for which the county exists.\n",
    "\n",
    "    # Get the name of all files in \"collections\" directory\n",
    "    allCollections = np.array(os.listdir(pathData))\n",
    "\n",
    "    # Get the ones relevant to the current amt\n",
    "    curCollectionsFilenames = allCollections[[int(x.split('_')[0]) == amtID for x in allCollections]]\n",
    "\n",
    "    # Extract start and end dates\n",
    "    allStarts = [x.split('_')[2] for x in curCollectionsFilenames]\n",
    "    allEnds = [x.split('_')[3].split('.')[0] for x in curCollectionsFilenames]\n",
    "\n",
    "    # Make a list of all dataframes\n",
    "    alldfs = []\n",
    "    for filename in curCollectionsFilenames:\n",
    "        # Read the next file\n",
    "        curdf = pd.read_csv(pathData + filename)\n",
    "        # Append to list\n",
    "        alldfs.append(curdf)\n",
    "    \n",
    "    return alldfs,allStarts,allEnds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags and analysis parameters\n",
    "numYears = 12 # Number of years on both sides of date to use for baseline calculations \n",
    "numYearsTot = (numYears*2) # The \"name\" of the baseline (i.e. +/- 5 years is a 10-year baseline, +/- 12 is a 24 year baseline)\n",
    "thresholdExcess = 3 # Threshold (in terms of Z-scores) for identifying a day as having increased excess\n",
    "\n",
    "# # For sensitivity analyses\n",
    "# numYears = 6 # Number of years on both sides of date to use for baseline calculations \n",
    "# # numYears = 9 # Number of years on both sides of date to use for baseline calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agegroups to analyze\n",
    "ageGroups = [\n",
    "    ['Total'],\n",
    "    ['Stillborn','0'],\n",
    "    ['1-4','5-9', '10-14'],\n",
    "    ['15-19', '20-24', '25-29', '30-34', '35-39'],\n",
    "    ['40-44', '45-49', '50-54', '55-59'],\n",
    "    ['60-64', '65-69', '70-74', '75-79', '80+']\n",
    "]\n",
    "\n",
    "# And the names to use for directories and filenames\n",
    "ageGroupNames = [\n",
    "    'Total',\n",
    "    'Infants_stillborn',\n",
    "    '1-14',\n",
    "    '15-39',\n",
    "    '40-59',\n",
    "    '60+'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for summing columns\n",
    "def sumColumns(curdf,columnsToUse=['Total']):\n",
    "    # Returns the sum of the columns specified\n",
    "    return curdf[columnsToUse].sum(axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pathToSaveResultsInUpper = pathToSaveResultsIn + f'_Years{numYears}_Threshold{thresholdExcess}/'\n",
    "\n",
    "# # Create directory if it doesn't already exist\n",
    "# try:\n",
    "#     os.mkdir(pathToSaveResultsInUpper)\n",
    "#     print('Created directory')\n",
    "# except:\n",
    "#     2+2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# ##### Run analysis for each of the agegroups #####\n",
    "# ############ Takes about half an hour ############ \n",
    "# ##################################################\n",
    "\n",
    "# # Prepare progressbar and go through each county\n",
    "# pbar = tqdm(allAmtIDs)\n",
    "# for curAmtID in pbar:\n",
    "\n",
    "#     # Get county name, data and periods\n",
    "#     curAmtName = ps.getAmtName(curAmtID)\n",
    "#     alldfs,allStarts,allEnds = ps.getAmtCollections(curAmtID)\n",
    "\n",
    "#     # Go through each possible period\n",
    "#     for i in range(len(allStarts)):\n",
    "\n",
    "#         # Get the dataframe, start date and end date\n",
    "#         curdf = alldfs[i].copy()\n",
    "#         curStart = allStarts[i]\n",
    "#         curEnd = allEnds[i]\n",
    "\n",
    "#         # Ensure date is datetime\n",
    "#         curdf['Date'] = pd.to_datetime(curdf.Date)\n",
    "#         # Set date as index\n",
    "#         curdf = curdf.set_index('Date')\n",
    "        \n",
    "#         # Prepare dataframe to save to file\n",
    "#         dfToSave = curdf.copy()\n",
    "\n",
    "#         for ageIndex in range(len(ageGroups)):\n",
    "#             # Get current agegroups and name\n",
    "#             curAgeGroup = ageGroups[ageIndex]\n",
    "#             curAgeName = ageGroupNames[ageIndex]\n",
    "\n",
    "#             # Update progressbar\n",
    "#             pbar.set_postfix(\n",
    "#                 {\n",
    "#                     'Amt':curAmtName,\n",
    "#                     'Period':i+1,\n",
    "#                     'Total periods':len(allStarts),\n",
    "#                     'Start':curStart,\n",
    "#                     'End':curEnd,\n",
    "#                     'Agegroup':curAgeName,\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "\n",
    "#             # Sum the columns of the given agegroups\n",
    "#             curSeries = sumColumns(curdf,curAgeGroup)\n",
    "\n",
    "#             # Calculate the 7-day average to avoid trouble with Sundays having more burials than other weekdays\n",
    "#             curSeriesRn = curSeries.rolling(window=7,center=True).mean()\n",
    "\n",
    "#             # this_curTime,this_curVals,this_corrMean,this_corrStd,this_postResi,this_postResiStd,this_postResiPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "#             # curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct\n",
    "#             curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "            \n",
    "#             # Make a dataframe for results\n",
    "#             dfResults = pd.DataFrame(\n",
    "#                 {\n",
    "#                     # curAgeName+'_Data':curSeries,\n",
    "#                     curAgeName+'_Data7DayMean':curSeriesRn,\n",
    "#                     curAgeName+'_Baseline':curBaseline, \n",
    "#                     curAgeName+'_StandardDeviation':curStandardDeviation,\n",
    "#                     curAgeName+'_Zscore':curZscore, \n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             # Merge with data and results-so-far\n",
    "#             dfToSave = pd.merge(dfToSave,dfResults, left_index=True, right_index=True)\n",
    "    \n",
    "#         # Determine filename to save file as\n",
    "#         curFileName =  str(int(curAmtID)) + '_'+curAmtName + '_'+pd.to_datetime(curStart).strftime('%Y-%m-%d') +'_'+pd.to_datetime(curEnd).strftime('%Y-%m-%d')\n",
    "#         # curFileName = curFileName + '_'+curAgeName+'.csv'\n",
    "#         curFileName = curFileName + '.csv'\n",
    "        \n",
    "#         # # Save county results to file\n",
    "#         # dfToSave.reset_index().to_csv('test.csv')\n",
    "#         # Only save analysis results, to save space in github repo\n",
    "#         # dfToSave.iloc[:,curdf.shape[1]:].reset_index().to_csv(pathToSaveResultsInUpper + curFileName,index=False) # Saves everything, with very high float precision\n",
    "#         dfToSave.iloc[:,curdf.shape[1]:].reset_index().round(7).to_csv(pathToSaveResultsInUpper + curFileName,index=False) # Rounds everything to 7 decimals before saving, reduces filesize significantly compared to the line above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without smoothing with 7-days mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pathToSaveResultsInUpperNonSmooth = pathToSaveResultsIn + f'_NonSmoothed_Years{numYears}_Threshold{thresholdExcess}/'\n",
    "\n",
    "# Create directory if it doesn't already exist\n",
    "try:\n",
    "    os.mkdir(pathToSaveResultsInUpperNonSmooth)\n",
    "    print('Created directory')\n",
    "except:\n",
    "    2+2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\NonSyncedFiles\\GithubRepos\\SignatureFeatures\\Code\\AnalyzeExcessMortality.ipynb Cell 16\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(allAmtIDs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m curAmtID \u001b[39min\u001b[39;00m pbar:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Get county name, data and periods\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     curAmtName \u001b[39m=\u001b[39m ps\u001b[39m.\u001b[39mgetAmtName(curAmtID)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     alldfs,allStarts,allEnds \u001b[39m=\u001b[39m ps\u001b[39m.\u001b[39mgetAmtCollections(curAmtID)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/NonSyncedFiles/GithubRepos/SignatureFeatures/Code/AnalyzeExcessMortality.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Go through each possible period\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ps' is not defined"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "##### Run analysis for each of the agegroups #####\n",
    "############ Takes about half an hour ############ \n",
    "##################################################\n",
    "\n",
    "# Prepare progressbar and go through each county\n",
    "pbar = tqdm(allAmtIDs)\n",
    "for curAmtID in pbar:\n",
    "\n",
    "    # Get county name, data and periods\n",
    "    curAmtName = getAmtName(curAmtID)\n",
    "    alldfs,allStarts,allEnds = getAmtCollections(curAmtID)\n",
    "\n",
    "    # Go through each possible period\n",
    "    for i in range(len(allStarts)):\n",
    "\n",
    "        # Get the dataframe, start date and end date\n",
    "        curdf = alldfs[i].copy()\n",
    "        curStart = allStarts[i]\n",
    "        curEnd = allEnds[i]\n",
    "\n",
    "        # Ensure date is datetime\n",
    "        curdf['Date'] = pd.to_datetime(curdf.Date)\n",
    "        # Set date as index\n",
    "        curdf = curdf.set_index('Date')\n",
    "        \n",
    "        # Prepare dataframe to save to file\n",
    "        dfToSave = curdf.copy()\n",
    "\n",
    "        for ageIndex in range(len(ageGroups)):\n",
    "            # Get current agegroups and name\n",
    "            curAgeGroup = ageGroups[ageIndex]\n",
    "            curAgeName = ageGroupNames[ageIndex]\n",
    "\n",
    "            # Update progressbar\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    'Amt':curAmtName,\n",
    "                    'Period':i+1,\n",
    "                    'Total periods':len(allStarts),\n",
    "                    'Start':curStart,\n",
    "                    'End':curEnd,\n",
    "                    'Agegroup':curAgeName,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Sum the columns of the given agegroups\n",
    "            curSeries = sumColumns(curdf,curAgeGroup)\n",
    "\n",
    "            # # Calculate the 7-day average to avoid trouble with Sundays having more burials than other weekdays\n",
    "            # curSeriesRn = curSeries.rolling(window=7,center=True).mean()\n",
    "\n",
    "            # this_curTime,this_curVals,this_corrMean,this_corrStd,this_postResi,this_postResiStd,this_postResiPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "            # curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct\n",
    "            # curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "            curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct = emf.runFullAnalysisDailySeries(curSeries,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "            \n",
    "            # Make a dataframe for results\n",
    "            dfResults = pd.DataFrame(\n",
    "                {\n",
    "                    curAgeName+'_Data':curSeries,\n",
    "                    # curAgeName+'_Data7DayMean':curSeriesRn,\n",
    "                    curAgeName+'_Baseline':curBaseline, \n",
    "                    curAgeName+'_StandardDeviation':curStandardDeviation,\n",
    "                    curAgeName+'_Zscore':curZscore, \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Merge with data and results-so-far\n",
    "            dfToSave = pd.merge(dfToSave,dfResults, left_index=True, right_index=True)\n",
    "    \n",
    "        # Determine filename to save file as    \n",
    "        curFileName =  str(int(curAmtID)) + '_'+curAmtName + '_'+pd.to_datetime(curStart).strftime('%Y-%m-%d') +'_'+pd.to_datetime(curEnd).strftime('%Y-%m-%d')\n",
    "        # curFileName = curFileName + '_'+curAgeName+'.csv'\n",
    "        curFileName = curFileName + '.csv'\n",
    "        \n",
    "        # # Save county results to file\n",
    "        # dfToSave.reset_index().to_csv('test.csv')\n",
    "        # Only save analysis results, to save space in github repo\n",
    "        # dfToSave.iloc[:,curdf.shape[1]:].reset_index().to_csv(pathToSaveResultsInUpperNonSmooth + curFileName,index=False) # Saves everything, with very high float precision\n",
    "        dfToSave.iloc[:,curdf.shape[1]:].reset_index().round(7).to_csv(pathToSaveResultsInUpperNonSmooth + curFileName,index=False) # Rounds everything to 7 decimals before saving, reduces filesize significantly compared to the line above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old version, saved age-specific results in separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfToSave.iloc[:,curdf.shape[1]:].reset_index().round(7).to_csv(pathToSaveResultsInUpper + 'test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# ##### Run analysis for each of the agegroups #####\n",
    "# ############ Takes about half an hour ############ \n",
    "# ##################################################\n",
    "\n",
    "# for ageIndex in range(len(ageGroups)):\n",
    "#     # Get current agegroups and name\n",
    "#     curAgeGroup = ageGroups[ageIndex]\n",
    "#     curAgeName = ageGroupNames[ageIndex]\n",
    "\n",
    "#     # Determine path to save results in \n",
    "#     pathToSaveResultsInLower = pathToSaveResultsInUpper + '/Age_' + curAgeName+'/'\n",
    "\n",
    "#     # Create subdirectory\n",
    "#     try:\n",
    "#         os.mkdir(pathToSaveResultsInLower)\n",
    "#     except:\n",
    "#         2+2\n",
    "\n",
    "#     # Prepare progressbar and go through each county\n",
    "#     pbar = tqdm(allAmtIDs)\n",
    "#     for curAmtID in pbar:\n",
    "\n",
    "#         # Get county name, data and periods\n",
    "#         curAmtName = ps.getAmtName(curAmtID)\n",
    "#         alldfs,allStarts,allEnds = ps.getAmtCollections(curAmtID)\n",
    "\n",
    "#         # Go through each possible period\n",
    "#         for i in range(len(allStarts)):\n",
    "\n",
    "#             # Get the dataframe, start date and end date\n",
    "#             curdf = alldfs[i].copy()\n",
    "#             curStart = allStarts[i]\n",
    "#             curEnd = allEnds[i]\n",
    "\n",
    "#             # Update progressbar\n",
    "#             pbar.set_postfix(\n",
    "#                 {\n",
    "#                     'Agegroup':curAgeName,\n",
    "#                     'Amt':curAmtName,\n",
    "#                     'Period':i,\n",
    "#                     'Total periods':len(allStarts),\n",
    "#                     'Start':curStart,\n",
    "#                     'End':curEnd,\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Ensure date is datetime\n",
    "#             curdf['Date'] = pd.to_datetime(curdf.Date)\n",
    "#             # Set date as index\n",
    "#             curdf = curdf.set_index('Date')\n",
    "\n",
    "#             # Sum the columns of the given agegroups\n",
    "#             curSeries = sumColumns(curdf,curAgeGroup)\n",
    "\n",
    "#             # Calculate the 7-day average to avoid trouble with Sundays having more burials than other weekdays\n",
    "#             curSeriesRn = curSeries.rolling(window=7,center=True).mean()\n",
    "\n",
    "#             # this_curTime,this_curVals,this_corrMean,this_corrStd,this_postResi,this_postResiStd,this_postResiPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "#             # curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct\n",
    "#             curBaseline,curStandardDeviation,curExcess,curZscore,curExcessPct = emf.runFullAnalysisDailySeries(curSeriesRn,numYears=numYears,ZscoreThreshold=thresholdExcess)\n",
    "            \n",
    "            \n",
    "#             # Make a dataframe for results\n",
    "#             dfResults = pd.DataFrame(\n",
    "#                 {\n",
    "#                     'Data':curSeries,\n",
    "#                     'DataSmooth':curSeriesRn,\n",
    "#                     'Baseline':curBaseline, \n",
    "#                     'StandardDeviation':curStandardDeviation,\n",
    "#                     'Zscore':curZscore, \n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "\n",
    "#             # Determine filename to save file as\n",
    "#             curFileName =  str(int(curAmtID)) + '_'+curAmtName + '_'+pd.to_datetime(curStart).strftime('%Y-%m-%d') +'_'+pd.to_datetime(curEnd).strftime('%Y-%m-%d')\n",
    "#             curFileName = curFileName + '_'+curAgeName+'.csv'\n",
    "\n",
    "#             # Save to file (reset index to also save date to file)\n",
    "#             dfResults.reset_index().to_csv(pathToSaveResultsInLower+curFileName,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1231565b209786dac476dcddb6c851658cee15112262a0f5b5be5fa490b6ca1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
